{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Forecasting ML Pipeline [manual]\n",
    "\n",
    "---\n",
    "\n",
    "Once you are familiar with using Amazon SageMaker built-in algorithm - [DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) to do [air quality forecasting model traing](./01_train_and_evaluate_air_quality_deepar_model.ipynb), we are going to build a ML Pipeline to automate the workflow with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). \n",
    "\n",
    "First and foremost, let's have a quick dive-in to the architecture design.\n",
    "\n",
    "![Air Quality Forecasting Architecture Design](./img/air_quality_forecasting_architecture.png)\n",
    "\n",
    "In the architecture diagram:\n",
    "* There is preprocessing job to do data integration\n",
    "  * A table is created in Amazon Athena to query data on open air quality data. Visit [Open AQ](https://openaq.org/) for detail.\n",
    "  * A query to Amazon Athena to collect Sydney, Australia air quality data.\n",
    "  * Data cleansing and feature engineering\n",
    "  * Train and test data set are separated; we keep last 30 days' data as test set.\n",
    "  * Batch Transform test data is constructed based on the latest 100 record in test set. \n",
    "* Hyperparameters optimization is optional\n",
    "  * In pipeline, we will leave hyperparameter optimziation alone without doing batch transform.\n",
    "* Model training with tuned hyperparameters\n",
    "  * For example, you may collect the hyperparameters from HPO jobs with the best candidate.\n",
    "* Batch transform job is triggerred to forecast air quality.\n",
    "  * In the example, we do the batch inference for the latest 100 records in our test data set. \n",
    "\n",
    "In the notebook, we are going to demo how to create the workflow step by step and process till model training. Below is the related Step Functions workflow mapping to the ML pipeline with no HPO and using an trained model:\n",
    "\n",
    "![Air Quality Forecasting ML Pipeline](./img/air_quality_forecasting_ml_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Creation\n",
    "---\n",
    "To create ML pipeline, we will use Step Functions Data Science SDK v2.0.0rc1 version, which is compatible with SageMaker SDK 2.x.x.\n",
    "\n",
    "In below cells, we will demo:\n",
    "* Environment initialization\n",
    "* Preprocessing docker container creation\n",
    "* ML Pipeline creation and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached pip-20.2.3-py2.py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Uninstalling pip-20.0.2:\n",
      "      Successfully uninstalled pip-20.0.2\n",
      "Successfully installed pip-20.2.3\n",
      "Name: sagemaker\n",
      "Version: 2.15.0\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: None\n",
      "License: Apache License 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages\n",
      "Requires: smdebug-rulesconfig, boto3, protobuf, protobuf3-to-dict, numpy, packaging, google-pasta, importlib-metadata\n",
      "Required-by: stepfunctions\n",
      "---\n",
      "Name: stepfunctions\n",
      "Version: 2.0.0rc1\n",
      "Summary: Open source library for develping data science workflows on AWS Step Functions.\n",
      "Home-page: https://github.com/aws/aws-step-functions-data-science-sdk-python\n",
      "Author: Amazon Web Services\n",
      "Author-email: None\n",
      "License: Apache License 2.0\n",
      "Location: /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages\n",
      "Requires: boto3, pyyaml, sagemaker\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" # 2.0.0\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0rc1\"\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include imports, bucket_name, role and existing training model uri.\n",
    "from pipeline.ml_pipeline_dependencies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the workflow execution role. For the role arn, please refer to the output tab of the CloudFormation stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = boto3.client('ssm')\n",
    "response = ssm.get_parameter(Name = \"/ml_pipeline/workflow_execution_role\")\n",
    "WORKFLOW_EXECUTION_ROLE = response['Parameter']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow execution IAM service role: arn:aws:iam::593380422482:role/ml-pipeline-afq-StepFunctionsWorkflowExecutionRole-TIAHOXUZPC3W\n"
     ]
    }
   ],
   "source": [
    "if not WORKFLOW_EXECUTION_ROLE:\n",
    "    raise Exception(\"ML Pipeline Parameters in System Manager is not setup properly. Please check whether the ml-pipeline stack has been created or not.\")\n",
    "else:\n",
    "    print(f\"Workflow execution IAM service role: {WORKFLOW_EXECUTION_ROLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image for SageMaker Processing\n",
    "\n",
    "Define your own processing container and install related dependencies.\n",
    "\n",
    "Below, we walk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container. Create a container support data preprocessing, feature engineering and model evaluation. \n",
    "\n",
    "This block of code buils the container using the docker command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define repository name and uri variables\n",
    "ecr_repository = 'air-quality-forecasting-preprocessing'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  21.76MB\n",
      "Step 1/5 : FROM python:3.7-slim-buster\n",
      "3.7-slim-buster: Pulling from library/python\n",
      "\n",
      "\u001b[1Bb6b2107f: Pulling fs layer \n",
      "\u001b[1B0c3f3e2b: Pulling fs layer \n",
      "\u001b[1Bdaf7eb83: Pulling fs layer \n",
      "\u001b[1B495f5be2: Pulling fs layer \n",
      "\u001b[1B402a4251: Pull complete 408MB/2.408MBB\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:de75b6dc5e4040a5910f923bdeb9a63e050ed1e62844a886f44e8c02f4f80af0\n",
      "Status: Downloaded newer image for python:3.7-slim-buster\n",
      " ---> c55447e52803\n",
      "Step 2/5 : COPY ./sql /opt/ml/processing/sql\n",
      " ---> 0ce54fe861bd\n",
      "Step 3/5 : RUN pip install pandas numpy geopandas scikit-learn fsspec s3fs \"boto3==1.14.44\" \"botocore==1.17.44\"\n",
      " ---> Running in e0423aa5c642\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-0.8.1-py2.py3-none-any.whl (962 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-0.8.3-py3-none-any.whl (88 kB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "Collecting boto3==1.14.44\n",
      "  Downloading boto3-1.14.44-py2.py3-none-any.whl (129 kB)\n",
      "Collecting botocore==1.17.44\n",
      "  Downloading botocore-1.17.44-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Downloading pyproj-2.6.1.post1-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
      "Collecting shapely\n",
      "  Downloading Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting fiona\n",
      "  Downloading Fiona-1.8.17-cp37-cp37m-manylinux1_x86_64.whl (14.8 MB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.1.2-py3-none-any.whl (45 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting urllib3<1.26,>=1.20; python_version != \"3.4\"\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.5.0-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs>=17\n",
      "  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
      "Collecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting click<8,>=4.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.7.0-py3-none-any.whl (20 kB)\n",
      "Collecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting aiohttp>=3.3.1\n",
      "  Downloading aiohttp-3.6.3-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting typing_extensions>=3.7\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting yarl<1.6.0,>=1.0\n",
      "  Downloading yarl-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (258 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<5.0,>=4.5\n",
      "  Downloading multidict-4.7.6-cp37-cp37m-manylinux1_x86_64.whl (149 kB)\n",
      "Collecting chardet<4.0,>=2.0\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna>=2.0\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19552 sha256=1d5f819154469574f0a8239cf3cf2151935cfc1f441c814a176cd76811924104\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built wrapt\n",
      "Installing collected packages: six, python-dateutil, numpy, pytz, pandas, pyproj, shapely, click, cligj, attrs, click-plugins, munch, fiona, geopandas, scipy, joblib, threadpoolctl, scikit-learn, fsspec, typing-extensions, aioitertools, jmespath, urllib3, docutils, botocore, wrapt, idna, multidict, yarl, async-timeout, chardet, aiohttp, aiobotocore, s3fs, s3transfer, boto3\n",
      "Successfully installed aiobotocore-1.1.2 aiohttp-3.6.3 aioitertools-0.7.0 async-timeout-3.0.1 attrs-20.2.0 boto3-1.14.44 botocore-1.17.44 chardet-3.0.4 click-7.1.2 click-plugins-1.1.1 cligj-0.5.0 docutils-0.15.2 fiona-1.8.17 fsspec-0.8.3 geopandas-0.8.1 idna-2.10 jmespath-0.10.0 joblib-0.17.0 multidict-4.7.6 munch-2.5.0 numpy-1.19.2 pandas-1.1.3 pyproj-2.6.1.post1 python-dateutil-2.8.1 pytz-2020.1 s3fs-0.5.1 s3transfer-0.3.3 scikit-learn-0.23.2 scipy-1.5.2 shapely-1.7.1 six-1.15.0 threadpoolctl-2.1.0 typing-extensions-3.7.4.3 urllib3-1.25.10 wrapt-1.12.1 yarl-1.5.1\n",
      "Removing intermediate container e0423aa5c642\n",
      " ---> fb7571829b2f\n",
      "Step 4/5 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Running in 5fe8144fc130\n",
      "Removing intermediate container 5fe8144fc130\n",
      " ---> d61c3084c600\n",
      "Step 5/5 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in 1db57afc3406\n",
      "Removing intermediate container 1db57afc3406\n",
      " ---> 89bcc71e521a\n",
      "Successfully built 89bcc71e521a\n",
      "Successfully tagged air-quality-forecasting-preprocessing:latest\n"
     ]
    }
   ],
   "source": [
    "# build the image.\n",
    "!docker build -t $ecr_repository -f ./pipeline/ml_pipeline_preprocessing_Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'air-quality-forecasting-preprocessing' already exists in the registry with id '593380422482'\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [593380422482.dkr.ecr.ap-southeast-2.amazonaws.com/air-quality-forecasting-preprocessing]\n",
      "\n",
      "\u001b[1B5ab4c5b4: Preparing \n",
      "\u001b[1Bb3bd6bfa: Preparing \n",
      "\u001b[1Bd015701f: Preparing \n",
      "\u001b[1B92983f67: Preparing \n",
      "\u001b[1Bab5b57f8: Preparing \n",
      "\u001b[1Bf82ca30a: Preparing \n",
      "\u001b[7B5ab4c5b4: Pushed   476.7MB/467.8MBA\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:75f73825b6f18ceec78aa73be4ff1cfc5cc34c23e2fcb88783a6c3a7995b9916 size: 1790\n"
     ]
    }
   ],
   "source": [
    "# ECR repository should have been created with CloudFormation stack. Uncomment below to create it in case it wasn't.\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "\n",
    "# Login and push the built docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ProcessingStep\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "In the processing job script `./pipeline/ml_pipeline_preprocessing.py`, the actions will be done:\n",
    "\n",
    "* Create Athena table with external source - OpenAQ\n",
    "* Query Sydney OpenAQ data \n",
    "* Feature engineering on the dataset\n",
    "* Split training and test data \n",
    "* Store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"./pipeline/ml_pipeline_preprocessing.py\"\n",
    "input_code_uri = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = bucket_name,\n",
    "    key_prefix = \"preprocessing/code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_processor = ScriptProcessor(\n",
    "    command = ['python3'],\n",
    "    image_uri = processing_repository_uri,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.2xlarge',\n",
    "    max_runtime_in_seconds = 1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output with training, test & all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = f\"s3://{bucket_name}/preprocessing/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will use ScriptProcessor as defined in previous steps along with the inputs and outputs objects that are defined in the below steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        source = input_code_uri,\n",
    "        destination = \"/opt/ml/processing/input/code\",\n",
    "        input_name = \"code\"\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output/all\",\n",
    "        destination = f\"{output_data}/all\",\n",
    "        output_name = \"all_data\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output/train\",\n",
    "        destination = f\"{output_data}/train\",\n",
    "        output_name = \"train_data\"\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output/test\",\n",
    "        destination = f\"{output_data}/test\",\n",
    "        output_name = \"test_data\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Execution parameters\n",
    "execution_input = ExecutionInput(\n",
    "    schema = {\n",
    "        \"PreprocessingJobName\": str,\n",
    "        \"ToDoHPO\": bool,\n",
    "        \"ToDoTraining\": bool,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TuningJobName\": str,\n",
    "        \"ModelName\": str,\n",
    "        \"EndpointName\": str,\n",
    "        \"TransformJobName\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProcessingStep` queries open air quality data for Sydney Australia with Amazon Athena. Especially, we are using our bucket to store query result. In case you setup default workgroup in Amazon Athena, please ensure to uncheck ***Override client-side settings***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"AirQualityForecasting Preprocessing Step\",\n",
    "    processor = preprocessing_processor,\n",
    "    job_name = execution_input[\"PreprocessingJobName\"],\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    container_arguments = [\"--split-days\", \"30\", \"--region\", region, \"--bucket-name\", bucket_name],\n",
    "    container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/ml_pipeline_preprocessing.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Setup tuning step and use choice state to decide whether we should do HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuning_output_path = f's3://{bucket_name}/sagemaker/tuning/output'\n",
    "image_uri = sagemaker.image_uris.retrieve('forecasting-deepar', region, '1')\n",
    "ml_instance_type = 'ml.g4dn.8xlarge'\n",
    "\n",
    "tuning_estimator = sagemaker.estimator.Estimator(\n",
    "        sagemaker_session = sagemaker_session,\n",
    "        image_uri = image_uri,\n",
    "        role = role,\n",
    "        instance_count = 1,\n",
    "        instance_type = ml_instance_type,\n",
    "        base_job_name = 'deepar-openaq-demo',\n",
    "        output_path = tuning_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set static hyperparameters\n",
    "The static parameters are the ones we know to be the best based on previously run HPO jobs, as well as the non-tunable parameters like prediction length and time frequency that are set according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = dict(\n",
    "    time_freq= '1H'\n",
    "    ,early_stopping_patience= 40\n",
    "    ,prediction_length= 48\n",
    "    ,num_eval_samples= 10\n",
    "\n",
    "    # default quantiles [0.1, 0.2, 0.3, ..., 0.9] is used\n",
    "    #,test_quantiles= quantiles\n",
    "    \n",
    "    # not setting these since HPO will use range of values\n",
    "    #,epochs= 400\n",
    "    #,context_length= 3\n",
    "    #,num_cells= 157\n",
    "    #,num_layers= 4\n",
    "    #,dropout_rate= 0.04\n",
    "    #,embedding_dimension= 12\n",
    "    #,mini_batch_size= 633\n",
    "    #,learning_rate= 0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyper-parameter ranges\n",
    "The hyperparameter ranges define the parameters we want the runer to search across.\n",
    "\n",
    "> Explore: Look in the [user guide](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html) for DeepAR and add the recommended ranges for `embedding_dimension` to the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_ranges = dict(\n",
    "    epochs= IntegerParameter(1, 1000)\n",
    "    ,context_length= IntegerParameter(7, 48)\n",
    "    ,num_cells= IntegerParameter(30,200)\n",
    "    ,num_layers= IntegerParameter(1,8)\n",
    "    ,dropout_rate= ContinuousParameter(0.0, 0.2)\n",
    "    ,embedding_dimension= IntegerParameter(1, 50)\n",
    "    ,mini_batch_size= IntegerParameter(32, 1028)\n",
    "    ,learning_rate= ContinuousParameter(.00001, .1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HPO tunning job step\n",
    "Once we have the HPO tuner defined, we can define the tuning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_estimator.set_hyperparameters(**hpo)\n",
    "\n",
    "hpo_tuner = HyperparameterTuner(\n",
    "    estimator = tuning_estimator, \n",
    "    objective_metric_name = 'train:final_loss',\n",
    "    objective_type = 'Minimize',\n",
    "    hyperparameter_ranges = hpo_ranges,\n",
    "    max_jobs = 2,\n",
    "    max_parallel_jobs = 1\n",
    ")\n",
    "\n",
    "hpo_data = dict(\n",
    "    train = f\"{output_data}/train\",\n",
    "    test = f\"{output_data}/test\"\n",
    ")\n",
    "# as long as HPO is selected, wait for completion.\n",
    "tuning_step = TuningStep(\n",
    "    \"HPO Step\",\n",
    "    tuner = hpo_tuner,\n",
    "    job_name = execution_input[\"TuningJobName\"],\n",
    "    data = hpo_data,\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We create a DeepAR instance, which we will use to run a training job. This will be used to create a TrainingStep for the workflow.\n",
    "\n",
    "#### Setup the training job step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output_path = f's3://{bucket_name}/sagemaker/training/output'\n",
    "training_estimator = sagemaker.estimator.Estimator(\n",
    "        sagemaker_session = sagemaker_session,\n",
    "        image_uri = image_uri,\n",
    "        role = role,\n",
    "        instance_count = 1,\n",
    "        instance_type = ml_instance_type,\n",
    "        base_job_name = 'deepar-openaq-demo',\n",
    "        output_path = training_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyper parameters for tuning\n",
    "hpo = dict(\n",
    "    time_freq= '1H'\n",
    "    ,early_stopping_patience= 40\n",
    "    ,prediction_length= 48\n",
    "    ,num_eval_samples= 10\n",
    "    #,test_quantiles= quantiles\n",
    "    ,epochs= 400\n",
    "    ,context_length= 3\n",
    "    ,num_cells= 157\n",
    "    ,num_layers= 4\n",
    "    ,dropout_rate= 0.04\n",
    "    ,embedding_dimension= 12\n",
    "    ,mini_batch_size= 633\n",
    "    ,learning_rate= 0.0005\n",
    ")\n",
    "training_estimator.set_hyperparameters(**hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all the features for training.\n",
    "data = dict(train = f\"{output_data}/all/all_features.json\")\n",
    "training_step = TrainingStep(\n",
    "    \"Training Step\",\n",
    "    estimator = training_estimator,\n",
    "    data = data,\n",
    "    job_name = execution_input[\"TrainingJobName\"],\n",
    "    wait_for_completion = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Step\n",
    "\n",
    "In the following cell, we define a model step that will create a model in Amazon SageMaker using the artifacts created during the TrainingStep. See  [ModelStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.ModelStep) in the AWS Step Functions Data Science SDK documentation to learn more.\n",
    "\n",
    "The model creation step typically follows the training step. The Step Functions SDK provides the [get_expected_model](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.TrainingStep.get_expected_model) method in the TrainingStep class to provide a reference for the trained model artifacts. Please note that this method is only useful when the ModelStep directly follows the TrainingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = ModelStep(\n",
    "    \"Save Model\",\n",
    "    model = training_step.get_expected_model(),\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    result_path = \"$.ModelStepResults\"\n",
    ")\n",
    "\n",
    "# for deploying existing model\n",
    "existing_model_name = f\"aqf-model-{uuid.uuid1().hex}\"\n",
    "existing_model = Model(\n",
    "    model_data = EXISTING_MODEL_URI,\n",
    "    image_uri = image_uri,\n",
    "    role = role,\n",
    "    name = existing_model_name\n",
    ")\n",
    "existing_model_step = ModelStep(\n",
    "    \"Existing Model\",\n",
    "    model = existing_model,\n",
    "    model_name = execution_input[\"ModelName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint Configuration Step\n",
    "In the following cell we create an endpoint configuration step. See [EndpointConfigStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/sagemaker.html#stepfunctions.steps.sagemaker.EndpointConfigStep) in the AWS Step Functions Data Science SDK documentation to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.c5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Endpoint Step\n",
    "In the following cells, we create the Endpoint step to deploy the new model as a managed API endpoint, updating an existing SageMaker endpoint if our choice state is sucessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = EndpointStep(\n",
    "    \"Model Endpoint\",\n",
    "    endpoint_name = execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name = execution_input[\"ModelName\"],\n",
    "    update = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform Step\n",
    "In the following cells, we create the Batch Transform step to do batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we only check '0.5' quatiles predictions.\n",
    "environment_param = {\n",
    "    'num_samples': 20,\n",
    "    'output_types': ['quantiles'],\n",
    "    'quantiles': ['0.5']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    execution_input[\"ModelName\"],\n",
    "    1,\n",
    "    'ml.c5.2xlarge',\n",
    "    output_path=f's3://{bucket_name}/sagemaker/batch_transform/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    strategy='MultiRecord',\n",
    "    assemble_with='Line',\n",
    "    env = {\n",
    "        'DEEPAR_INFERENCE_CONFIG': json.dumps(environment_param)\n",
    "    }\n",
    ")\n",
    "\n",
    "transformStep = TransformStep(\n",
    "    state_id = \"Batch Transform Step\",\n",
    "    transformer = transformer,\n",
    "    job_name = execution_input[\"TransformJobName\"],\n",
    "    model_name = execution_input[\"ModelName\"],\n",
    "    data = f\"{output_data}/test/batch_transform_test.json\",\n",
    "    split_type = 'Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup workflow process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Fail` state to mark the workflow failed in case any of the steps fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_pipeline_step_failure = Fail(\n",
    "    \"ML Workflow Failed\", cause = \"SageMakerPipelineStepFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = Chain([training_step, model_step, transformStep])\n",
    "deploy_existing_model_path = Chain([existing_model_step, transformStep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice state\n",
    "\n",
    "Now, we need to setup choice state for choose HPO / Training or not. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_choice = Choice(\n",
    "    \"To do HPO?\"\n",
    ")\n",
    "training_choice = Choice(\n",
    "    \"To do Model Training?\"\n",
    ")\n",
    "\n",
    "# refer to execution input variable with required format - not user friendly.\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = True),\n",
    "    next_step = tuning_step\n",
    ")\n",
    "hpo_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoHPO']\", value = False),\n",
    "    next_step = training_choice\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = True),\n",
    "    next_step = training_path\n",
    ")\n",
    "training_choice.add_choice(\n",
    "    rule = ChoiceRule.BooleanEquals(variable = \"$$.Execution.Input['ToDoTraining']\", value = False),\n",
    "    next_step = deploy_existing_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the Error handling in the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = Catch(\n",
    "    error_equals = [\"States.TaskFailed\"],\n",
    "    next_step = failed_state_sagemaker_pipeline_step_failure   \n",
    ")\n",
    "processing_step.add_catch(catch_state_processing)\n",
    "tuning_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)\n",
    "model_step.add_catch(catch_state_processing)\n",
    "endpoint_config_step.add_catch(catch_state_processing)\n",
    "endpoint_step.add_catch(catch_state_processing)\n",
    "existing_model_step.add_catch(catch_state_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution input parameter values\n",
    "preprocessing_job_name = f\"aqf-preprocessing-{uuid.uuid1().hex}\"\n",
    "tuning_job_name = f\"aqf-tuning-{uuid.uuid1().hex}\"\n",
    "training_job_name = f\"aqf-training-{uuid.uuid1().hex}\"\n",
    "model_job_name = f\"aqf-model-{uuid.uuid1().hex}\"\n",
    "endpoint_job_name = f\"aqf-endpoint-{uuid.uuid1().hex}\"\n",
    "batch_transform_job_name = f\"aqf-transform-{uuid.uuid1().hex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "WORKFLOW_NAME = \"manaul-aqf-ml-pipeline\"\n",
    "TO_DO_HPO = False\n",
    "TO_DO_TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflow_graph = Chain([processing_step, hpo_choice])\n",
    "workflow_graph = Chain([processing_step, hpo_choice])\n",
    "workflow = Workflow(\n",
    "    name = WORKFLOW_NAME,\n",
    "    definition = workflow_graph,\n",
    "    role = WORKFLOW_EXECUTION_ROLE\n",
    ")\n",
    "workflow.create()\n",
    "# update() to ensure existing workflow can get updated as create() just return ARN for the existing one.\n",
    "workflow.update(definition = workflow_graph) \n",
    "\n",
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"ToDoHPO\": TO_DO_HPO,\n",
    "        \"ToDoTraining\": TO_DO_TRAINING,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"ModelName\": model_job_name,\n",
    "        \"EndpointName\": endpoint_job_name,\n",
    "        \"TransformJobName\": batch_transform_job_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br>The Step Function workflow \"manaul-aqf-ml-pipeline\" is now executing... \n",
       "            <br>To view state machine in the console click \n",
       "            <a target=\"_blank\" href=\"https://ap-southeast-2.console.aws.amazon.com/states/home?region=ap-southeast-2#/statemachines/view/arn:aws:states:ap-southeast-2:593380422482:stateMachine:manaul-aqf-ml-pipeline\">State Machine</a> \n",
       "            <br>To view execution in the console click \n",
       "            <a target=\"_blank\" href=\"https://ap-southeast-2.console.aws.amazon.com/states/home?region=ap-southeast-2#/executions/details/arn:aws:states:ap-southeast-2:593380422482:execution:manaul-aqf-ml-pipeline:e93ef852-8f71-4db4-849d-73989e950f83\">Execution</a>.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = execution.describe()\n",
    "execution_id = response['name']\n",
    "# advice state machine console link\n",
    "display_state_machine_advice(WORKFLOW_NAME, execution_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell multiple times to observe the workflow execution progress. Please note that the execution may take 15-20mins with using existing model for batch transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-579\" class=\"workflowgraph\">\n",
       "    \n",
       "    <style>\n",
       "        .graph-legend ul {\n",
       "            list-style-type: none;\n",
       "            padding: 10px;\n",
       "            padding-left: 0;\n",
       "            margin: 0;\n",
       "            position: absolute;\n",
       "            top: 0;\n",
       "            background: transparent;\n",
       "        }\n",
       "\n",
       "        .graph-legend li {\n",
       "            margin-left: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend li > div {\n",
       "            width: 10px;\n",
       "            height: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend .success { background-color: #2BD62E }\n",
       "        .graph-legend .failed { background-color: #DE322F }\n",
       "        .graph-legend .cancelled { background-color: #DDDDDD }\n",
       "        .graph-legend .in-progress { background-color: #53C9ED }\n",
       "        .graph-legend .caught-error { background-color: #FFA500 }\n",
       "    </style>\n",
       "    <div class=\"graph-legend\">\n",
       "        <ul>\n",
       "            <li>\n",
       "                <div class=\"success\"></div>\n",
       "                <span>Success</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"failed\"></div>\n",
       "                <span>Failed</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"cancelled\"></div>\n",
       "                <span>Cancelled</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"in-progress\"></div>\n",
       "                <span>In Progress</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"caught-error\"></div>\n",
       "                <span>Caught Error</span>\n",
       "            </li>\n",
       "        </ul>\n",
       "    </div>\n",
       "\n",
       "    <svg></svg>\n",
       "    <a href=\"https://console.aws.amazon.com/states/home?region=ap-southeast-2#/executions/details/arn:aws:states:ap-southeast-2:593380422482:execution:manaul-aqf-ml-pipeline:e93ef852-8f71-4db4-849d-73989e950f83\" target=\"_blank\"> Inspect in AWS Step Functions </a>\n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-579')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 1000,\n",
       "        layout: 'TB',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"AirQualityForecasting Preprocessing Step\", \"States\": {\"AirQualityForecasting Preprocessing Step\": {\"Resource\": \"arn:aws:states:::sagemaker:createProcessingJob.sync\", \"Parameters\": {\"ProcessingJobName.$\": \"$$.Execution.Input['PreprocessingJobName']\", \"ProcessingInputs\": [{\"InputName\": \"code\", \"S3Input\": {\"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/code/ml_pipeline_preprocessing.py\", \"LocalPath\": \"/opt/ml/processing/input/code\", \"S3DataType\": \"S3Prefix\", \"S3InputMode\": \"File\", \"S3DataDistributionType\": \"FullyReplicated\", \"S3CompressionType\": \"None\"}}], \"ProcessingOutputConfig\": {\"Outputs\": [{\"OutputName\": \"all_data\", \"S3Output\": {\"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/all\", \"LocalPath\": \"/opt/ml/processing/output/all\", \"S3UploadMode\": \"EndOfJob\"}}, {\"OutputName\": \"train_data\", \"S3Output\": {\"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/train\", \"LocalPath\": \"/opt/ml/processing/output/train\", \"S3UploadMode\": \"EndOfJob\"}}, {\"OutputName\": \"test_data\", \"S3Output\": {\"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/test\", \"LocalPath\": \"/opt/ml/processing/output/test\", \"S3UploadMode\": \"EndOfJob\"}}]}, \"AppSpecification\": {\"ImageUri\": \"593380422482.dkr.ecr.ap-southeast-2.amazonaws.com/air-quality-forecasting-preprocessing:latest\", \"ContainerArguments\": [\"--split-days\", \"30\", \"--region\", \"ap-southeast-2\", \"--bucket-name\", \"openaq-forecasting-593380422482-ap-southeast-2\"], \"ContainerEntrypoint\": [\"python3\", \"/opt/ml/processing/input/code/ml_pipeline_preprocessing.py\"]}, \"RoleArn\": \"arn:aws:iam::593380422482:role/ml-pipeline-afq-SageMakerRole-171ODFWH3CTGT\", \"ProcessingResources\": {\"ClusterConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 30}}, \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 1200}}, \"Type\": \"Task\", \"Next\": \"To do HPO?\", \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ML Workflow Failed\"}]}, \"To do HPO?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$$.Execution.Input['ToDoHPO']\", \"BooleanEquals\": true, \"Next\": \"HPO Step\"}, {\"Variable\": \"$$.Execution.Input['ToDoHPO']\", \"BooleanEquals\": false, \"Next\": \"To do Model Training?\"}]}, \"HPO Step\": {\"Resource\": \"arn:aws:states:::sagemaker:createHyperParameterTuningJob.sync\", \"Parameters\": {\"HyperParameterTuningJobName.$\": \"$$.Execution.Input['TuningJobName']\", \"HyperParameterTuningJobConfig\": {\"Strategy\": \"Bayesian\", \"ResourceLimits\": {\"MaxNumberOfTrainingJobs\": 2, \"MaxParallelTrainingJobs\": 1}, \"TrainingJobEarlyStoppingType\": \"Off\", \"HyperParameterTuningJobObjective\": {\"Type\": \"Minimize\", \"MetricName\": \"train:final_loss\"}, \"ParameterRanges\": {\"ContinuousParameterRanges\": [{\"Name\": \"dropout_rate\", \"MinValue\": \"0.0\", \"MaxValue\": \"0.2\", \"ScalingType\": \"Auto\"}, {\"Name\": \"learning_rate\", \"MinValue\": \"1e-05\", \"MaxValue\": \"0.1\", \"ScalingType\": \"Auto\"}], \"CategoricalParameterRanges\": [], \"IntegerParameterRanges\": [{\"Name\": \"epochs\", \"MinValue\": \"1\", \"MaxValue\": \"1000\", \"ScalingType\": \"Auto\"}, {\"Name\": \"context_length\", \"MinValue\": \"7\", \"MaxValue\": \"48\", \"ScalingType\": \"Auto\"}, {\"Name\": \"num_cells\", \"MinValue\": \"30\", \"MaxValue\": \"200\", \"ScalingType\": \"Auto\"}, {\"Name\": \"num_layers\", \"MinValue\": \"1\", \"MaxValue\": \"8\", \"ScalingType\": \"Auto\"}, {\"Name\": \"embedding_dimension\", \"MinValue\": \"1\", \"MaxValue\": \"50\", \"ScalingType\": \"Auto\"}, {\"Name\": \"mini_batch_size\", \"MinValue\": \"32\", \"MaxValue\": \"1028\", \"ScalingType\": \"Auto\"}]}}, \"TrainingJobDefinition\": {\"AlgorithmSpecification\": {\"TrainingImage\": \"514117268639.dkr.ecr.ap-southeast-2.amazonaws.com/forecasting-deepar:1\", \"TrainingInputMode\": \"File\"}, \"OutputDataConfig\": {\"S3OutputPath\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/sagemaker/tuning/output\"}, \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400}, \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.g4dn.8xlarge\", \"VolumeSizeInGB\": 30}, \"RoleArn\": \"arn:aws:iam::593380422482:role/ml-pipeline-afq-SageMakerRole-171ODFWH3CTGT\", \"InputDataConfig\": [{\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/train\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"train\"}, {\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/test\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"test\"}], \"StaticHyperParameters\": {\"time_freq\": \"1H\", \"early_stopping_patience\": \"40\", \"prediction_length\": \"48\", \"num_eval_samples\": \"10\"}}}, \"Type\": \"Task\", \"End\": true, \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ML Workflow Failed\"}]}, \"ML Workflow Failed\": {\"Cause\": \"SageMakerPipelineStepFailed\", \"Type\": \"Fail\"}, \"To do Model Training?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$$.Execution.Input['ToDoTraining']\", \"BooleanEquals\": true, \"Next\": \"Training Step\"}, {\"Variable\": \"$$.Execution.Input['ToDoTraining']\", \"BooleanEquals\": false, \"Next\": \"Existing Model\"}]}, \"Training Step\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification\": {\"TrainingImage\": \"514117268639.dkr.ecr.ap-southeast-2.amazonaws.com/forecasting-deepar:1\", \"TrainingInputMode\": \"File\"}, \"OutputDataConfig\": {\"S3OutputPath\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/sagemaker/training/output\"}, \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400}, \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.g4dn.8xlarge\", \"VolumeSizeInGB\": 30}, \"RoleArn\": \"arn:aws:iam::593380422482:role/ml-pipeline-afq-SageMakerRole-171ODFWH3CTGT\", \"InputDataConfig\": [{\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/all/all_features.json\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"train\"}], \"HyperParameters\": {\"time_freq\": \"1H\", \"early_stopping_patience\": \"40\", \"prediction_length\": \"48\", \"num_eval_samples\": \"10\", \"epochs\": \"400\", \"context_length\": \"3\", \"num_cells\": \"157\", \"num_layers\": \"4\", \"dropout_rate\": \"0.04\", \"embedding_dimension\": \"12\", \"mini_batch_size\": \"633\", \"learning_rate\": \"0.0005\"}, \"TrainingJobName.$\": \"$$.Execution.Input['TrainingJobName']\"}, \"Type\": \"Task\", \"Next\": \"Save Model\", \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ML Workflow Failed\"}]}, \"Save Model\": {\"ResultPath\": \"$.ModelStepResults\", \"Parameters\": {\"ExecutionRoleArn\": \"arn:aws:iam::593380422482:role/ml-pipeline-afq-SageMakerRole-171ODFWH3CTGT\", \"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"PrimaryContainer\": {\"Environment\": {}, \"Image\": \"514117268639.dkr.ecr.ap-southeast-2.amazonaws.com/forecasting-deepar:1\", \"ModelDataUrl.$\": \"$['ModelArtifacts']['S3ModelArtifacts']\"}}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Batch Transform Step\", \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ML Workflow Failed\"}]}, \"Batch Transform Step\": {\"Resource\": \"arn:aws:states:::sagemaker:createTransformJob.sync\", \"Parameters\": {\"TransformJobName.$\": \"$$.Execution.Input['TransformJobName']\", \"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"TransformInput\": {\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/test/batch_transform_test.json\"}}, \"SplitType\": \"Line\"}, \"TransformOutput\": {\"S3OutputPath\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/sagemaker/batch_transform/output\", \"AssembleWith\": \"Line\"}, \"TransformResources\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.c5.2xlarge\"}, \"BatchStrategy\": \"MultiRecord\", \"Environment\": {\"DEEPAR_INFERENCE_CONFIG\": \"{\\\"num_samples\\\": 20, \\\"output_types\\\": [\\\"quantiles\\\"], \\\"quantiles\\\": [\\\"0.5\\\"]}\"}}, \"Type\": \"Task\", \"End\": true}, \"Existing Model\": {\"Parameters\": {\"ExecutionRoleArn\": \"arn:aws:iam::593380422482:role/ml-pipeline-afq-SageMakerRole-171ODFWH3CTGT\", \"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"PrimaryContainer\": {\"Environment\": {}, \"Image\": \"514117268639.dkr.ecr.ap-southeast-2.amazonaws.com/forecasting-deepar:1\", \"ModelDataUrl\": \"s3://openaq-forecasting-593380422482-ap-southeast-2/sagemaker/model/model.tar.gz\"}}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Batch Transform Step\", \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"ML Workflow Failed\"}]}}};\n",
       "    var elementId = '#graph-579';\n",
       "    var events = { 'events': [{\"timestamp\": 1602646040.534, \"type\": \"ExecutionStarted\", \"id\": 1, \"previousEventId\": 0, \"executionStartedEventDetails\": {\"input\": \"{\\n    \\\"PreprocessingJobName\\\": \\\"aqf-preprocessing-29d7f24e0dcd11ebb3ca819c285d6df2\\\",\\n    \\\"ToDoHPO\\\": false,\\n    \\\"ToDoTraining\\\": false,\\n    \\\"TrainingJobName\\\": \\\"aqf-training-29d7fad20dcd11ebb3ca819c285d6df2\\\",\\n    \\\"TuningJobName\\\": \\\"aqf-tuning-29d7f8980dcd11ebb3ca819c285d6df2\\\",\\n    \\\"ModelName\\\": \\\"aqf-model-29d7fcd00dcd11ebb3ca819c285d6df2\\\",\\n    \\\"EndpointName\\\": \\\"aqf-endpoint-29d7feb00dcd11ebb3ca819c285d6df2\\\",\\n    \\\"TransformJobName\\\": \\\"aqf-transform-29d800860dcd11ebb3ca819c285d6df2\\\"\\n}\", \"inputDetails\": {\"truncated\": false}, \"roleArn\": \"arn:aws:iam::593380422482:role/ml-pipeline-afq-StepFunctionsWorkflowExecutionRole-TIAHOXUZPC3W\"}}, {\"timestamp\": 1602646040.569, \"type\": \"TaskStateEntered\", \"id\": 2, \"previousEventId\": 0, \"stateEnteredEventDetails\": {\"name\": \"AirQualityForecasting Preprocessing Step\", \"input\": \"{\\n    \\\"PreprocessingJobName\\\": \\\"aqf-preprocessing-29d7f24e0dcd11ebb3ca819c285d6df2\\\",\\n    \\\"ToDoHPO\\\": false,\\n    \\\"ToDoTraining\\\": false,\\n    \\\"TrainingJobName\\\": \\\"aqf-training-29d7fad20dcd11ebb3ca819c285d6df2\\\",\\n    \\\"TuningJobName\\\": \\\"aqf-tuning-29d7f8980dcd11ebb3ca819c285d6df2\\\",\\n    \\\"ModelName\\\": \\\"aqf-model-29d7fcd00dcd11ebb3ca819c285d6df2\\\",\\n    \\\"EndpointName\\\": \\\"aqf-endpoint-29d7feb00dcd11ebb3ca819c285d6df2\\\",\\n    \\\"TransformJobName\\\": \\\"aqf-transform-29d800860dcd11ebb3ca819c285d6df2\\\"\\n}\", \"inputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1602646040.569, \"type\": \"TaskScheduled\", \"id\": 3, \"previousEventId\": 2, \"taskScheduledEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"region\": \"ap-southeast-2\", \"parameters\": \"{\\\"ProcessingInputs\\\":[{\\\"InputName\\\":\\\"code\\\",\\\"S3Input\\\":{\\\"S3Uri\\\":\\\"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/code/ml_pipeline_preprocessing.py\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/input/code\\\",\\\"S3DataType\\\":\\\"S3Prefix\\\",\\\"S3InputMode\\\":\\\"File\\\",\\\"S3DataDistributionType\\\":\\\"FullyReplicated\\\",\\\"S3CompressionType\\\":\\\"None\\\"}}],\\\"ProcessingOutputConfig\\\":{\\\"Outputs\\\":[{\\\"OutputName\\\":\\\"all_data\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/all\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/all\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}},{\\\"OutputName\\\":\\\"train_data\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/train\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/train\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}},{\\\"OutputName\\\":\\\"test_data\\\",\\\"S3Output\\\":{\\\"S3Uri\\\":\\\"s3://openaq-forecasting-593380422482-ap-southeast-2/preprocessing/output/test\\\",\\\"LocalPath\\\":\\\"/opt/ml/processing/output/test\\\",\\\"S3UploadMode\\\":\\\"EndOfJob\\\"}}]},\\\"AppSpecification\\\":{\\\"ImageUri\\\":\\\"593380422482.dkr.ecr.ap-southeast-2.amazonaws.com/air-quality-forecasting-preprocessing:latest\\\",\\\"ContainerArguments\\\":[\\\"--split-days\\\",\\\"30\\\",\\\"--region\\\",\\\"ap-southeast-2\\\",\\\"--bucket-name\\\",\\\"openaq-forecasting-593380422482-ap-southeast-2\\\"],\\\"ContainerEntrypoint\\\":[\\\"python3\\\",\\\"/opt/ml/processing/input/code/ml_pipeline_preprocessing.py\\\"]},\\\"RoleArn\\\":\\\"arn:aws:iam::593380422482:role/ml-pipeline-afq-SageMakerRole-171ODFWH3CTGT\\\",\\\"ProcessingResources\\\":{\\\"ClusterConfig\\\":{\\\"InstanceCount\\\":1,\\\"InstanceType\\\":\\\"ml.m5.2xlarge\\\",\\\"VolumeSizeInGB\\\":30}},\\\"StoppingCondition\\\":{\\\"MaxRuntimeInSeconds\\\":1200},\\\"ProcessingJobName\\\":\\\"aqf-preprocessing-29d7f24e0dcd11ebb3ca819c285d6df2\\\",\\\"Tags\\\":[{\\\"Key\\\":\\\"MANAGED_BY_AWS\\\",\\\"Value\\\":\\\"STARTED_BY_STEP_FUNCTIONS\\\"}]}\"}}, {\"timestamp\": 1602646040.666, \"type\": \"TaskStarted\", \"id\": 4, \"previousEventId\": 3, \"taskStartedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\"}}, {\"timestamp\": 1602646040.876, \"type\": \"TaskSubmitted\", \"id\": 5, \"previousEventId\": 4, \"taskSubmittedEventDetails\": {\"resourceType\": \"sagemaker\", \"resource\": \"createProcessingJob.sync\", \"output\": \"{\\\"ProcessingJobArn\\\":\\\"arn:aws:sagemaker:ap-southeast-2:593380422482:processing-job/aqf-preprocessing-29d7f24e0dcd11ebb3ca819c285d6df2\\\",\\\"SdkHttpMetadata\\\":{\\\"AllHttpHeaders\\\":{\\\"x-amzn-RequestId\\\":[\\\"ee5900ff-2624-48cb-a5c3-48a18701ddec\\\"],\\\"Content-Length\\\":[\\\"134\\\"],\\\"Date\\\":[\\\"Wed, 14 Oct 2020 03:27:20 GMT\\\"],\\\"Content-Type\\\":[\\\"application/x-amz-json-1.1\\\"]},\\\"HttpHeaders\\\":{\\\"Content-Length\\\":\\\"134\\\",\\\"Content-Type\\\":\\\"application/x-amz-json-1.1\\\",\\\"Date\\\":\\\"Wed, 14 Oct 2020 03:27:20 GMT\\\",\\\"x-amzn-RequestId\\\":\\\"ee5900ff-2624-48cb-a5c3-48a18701ddec\\\"},\\\"HttpStatusCode\\\":200},\\\"SdkResponseMetadata\\\":{\\\"RequestId\\\":\\\"ee5900ff-2624-48cb-a5c3-48a18701ddec\\\"}}\", \"outputDetails\": {\"truncated\": false}}}] };\n",
       "\n",
       "    var graph = new sfn.StateMachineExecutionGraph(definition, events, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.render_progress(portrait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
